{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxvmsVPeAiml",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder learning\n",
        "_María Camila Vásquez Correa_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYteKdHCAimm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder, label_binarize\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8NFM8fEAims",
        "colab_type": "text"
      },
      "source": [
        "## Multilayer perceptron\n",
        "The class model receives a list of layers (with its respective activation function) along with the loss that is going to be used for the training to initialize.\n",
        "* The fit method performs the training on X and y using learning rate lr, a number of epochs epochs, a batch_size and stops given a tolerance tol.\n",
        "* The evaluate method gives the prediction for an X\n",
        "* The score method gives the cost function for an X and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-s4yQ9UAimt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, layers, loss):\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "    \n",
        "    def fit(self, X, y, lr, epochs, batch_size, tol,print_loss = True):\n",
        "        if self.loss.history:\n",
        "            self.initialize()\n",
        "        total_loss = np.inf\n",
        "        j = 0\n",
        "        test_loss = []\n",
        "        while j <= epochs and total_loss >= tol:\n",
        "            _loss = 0\n",
        "            batches = len(y)//batch_size\n",
        "            for i in range(batches):\n",
        "                y_pred = self.forward_pass(X[i*batch_size:(i+1)*batch_size])\n",
        "                loss_ = self.loss.forward_pass(y_pred,y[i*batch_size:(i+1)*batch_size])\n",
        "                loss_gradient = loss.backward_pass(y_pred, y[i*batch_size:(i+1)*batch_size])\n",
        "                self.backward_pass(loss_gradient)\n",
        "                self.update_parameters(lr)\n",
        "                _loss += loss_\n",
        "            total_loss = _loss/batches\n",
        "            y_pred_test = self.forward_pass(X_test)\n",
        "            test_loss.append(loss.forward_pass(X_test, y_pred_test))\n",
        "            loss.history.append(total_loss)\n",
        "            self.update_gradient_history()\n",
        "            if print_loss:\n",
        "                print(f'epoch {j}: loss {total_loss}') \n",
        "            j += 1\n",
        "        return(test_loss)\n",
        "    \n",
        "    def initialize(self):\n",
        "        for layer in self.layers:\n",
        "            if layer.trainable:\n",
        "                layer.initialize()\n",
        "        self.loss.initialize()\n",
        "    \n",
        "    def forward_pass(self, X):\n",
        "        x = X.copy()\n",
        "        for layer in self.layers:\n",
        "            a = layer.forward_pass(x)\n",
        "            x = a\n",
        "        return x\n",
        "    \n",
        "    def backward_pass(self, upstream_gradient):\n",
        "        for layer in reversed(self.layers):\n",
        "            upstream_gradient = layer.backward_pass(upstream_gradient)                \n",
        "        return upstream_gradient\n",
        "    \n",
        "    def update_parameters(self,lr):\n",
        "        for layer in self.layers:\n",
        "            if layer.trainable:\n",
        "                layer.update_parameters(lr)\n",
        "                \n",
        "    def update_gradient_history(self):\n",
        "        for layer in self.layers:\n",
        "            if layer.trainable:\n",
        "                layer.gradient_history.append([np.sum(layer.gradient_history_w),np.sum(layer.gradient_history_b)])\n",
        "                layer.gradient_history_w = []\n",
        "                layer.gradient_history_b = []\n",
        "    \n",
        "    def evaluate(self, X):\n",
        "        return self.forward_pass(X)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        y_pred = self.forward_pass(X)\n",
        "        loss = self.loss.forward_pass(y_pred,y)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8yCTulMAimz",
        "colab_type": "text"
      },
      "source": [
        "### Layer\n",
        "Each layer has its own initialization (in which we specify whether or not the layer is trainable) and has their forward and backward pass defined for the backpropagation algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3gaUpAAim0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward_pass(self, x):\n",
        "        pass\n",
        "    \n",
        "    def backward_pass(self, upstream_gradient):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trui4qpnAim4",
        "colab_type": "text"
      },
      "source": [
        "### Local layer\n",
        "Each local layer has the weights and bias for an input, and gives an output to be activated with any of the activation functions specified. It is a trainable layer, that means it has a method update_parameters in which the gradient descend algorithm is performed in each parameter (weights and bias) taking into account the local gradient stored "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmKfBfSKAim5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Local(Layer):\n",
        "    def __init__(self, input_size, output_size, bias = True):\n",
        "        Xstdd = 2 / (input_size + output_size)\n",
        "        self.weights = np.random.normal(loc = 0, scale = Xstdd, size = (input_size, output_size))\n",
        "        if bias:\n",
        "            self.bias = np.zeros(output_size)\n",
        "            self.has_bias = True\n",
        "        else:\n",
        "            self.has_bias = False\n",
        "        self.local_gradient = {}\n",
        "        self.x = None\n",
        "        self.trainable = True\n",
        "        self.gradient_history = []\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.gradient_history_w = []\n",
        "        self.gradient_history_b = []\n",
        "    \n",
        "    def initialize(self):\n",
        "        Xstdd = 2 / (self.input_size + self.output_size)\n",
        "        self.weights = np.random.normal(loc = 0, scale = Xstdd, size = (self.input_size, self.output_size))\n",
        "        if self.has_bias:\n",
        "            self.bias = np.zeros(self.output_size)\n",
        "        self.local_gradient = {}\n",
        "        self.gradient_history = []\n",
        "        self.gradient_history_w = []\n",
        "        self.gradient_history_b = []\n",
        "        \n",
        "    # x: (batch_size, input_size)\n",
        "    # w: (input_size, output_size)\n",
        "    def forward_pass(self,_x):\n",
        "        self.x = _x.copy()\n",
        "        return _x @ self.weights + self.bias\n",
        "    \n",
        "    def backward_pass(self,upstream_gradient):\n",
        "        dx = upstream_gradient @ self.weights.T\n",
        "        dw = self.x.T @ upstream_gradient\n",
        "        if self.has_bias:\n",
        "            db = np.sum(upstream_gradient, axis = 0)\n",
        "            self.local_gradient = {'dw': dw, 'db': db}\n",
        "            self.gradient_history_b.append(np.sum(db))\n",
        "        else:\n",
        "            self.local_gradient = {'dw': dw}\n",
        "        self.gradient_history_w.append(np.sum(dw))\n",
        "        return dx\n",
        "    \n",
        "    def update_parameters(self,lr):\n",
        "        self.weights = self.weights - lr*self.local_gradient['dw']\n",
        "        if self.has_bias:\n",
        "            self.bias = self.bias - lr*self.local_gradient['db']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76DKP-mPAinA",
        "colab_type": "text"
      },
      "source": [
        "### Activation layers\n",
        "Each activation layer has the forward and backward pass, an is not trainable. Currently available layers: Relu, Sigmoid and Hiperbolic Tangent (Tanh). For a linear activation, do not set any activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0gM6y58AinC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.trainable = False\n",
        "        \n",
        "    def forward_pass(self,_x):\n",
        "        self.x = _x.copy()\n",
        "        return 1 / (1 + np.exp(-_x))\n",
        "    \n",
        "    def backward_pass(self,upstream_gradient):\n",
        "        s_prime = (1 / (1 + np.exp(-self.x)))*(1-(1 / (1 + np.exp(-self.x))))\n",
        "        dx = upstream_gradient * s_prime\n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TadCGrB5AinH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu(Layer):\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.trainable = False\n",
        "        \n",
        "    def forward_pass(self,x):\n",
        "        self.x = x\n",
        "        return np.where(x > 0, x, 0)\n",
        "    \n",
        "    def backward_pass(self,upstream_gradient):\n",
        "        r_prime = (self.x > 0).astype(np.float32)\n",
        "        dx = upstream_gradient * r_prime\n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1dJ4SFrAinL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.trainable = False\n",
        "        \n",
        "    def forward_pass(self,x):\n",
        "        self.x = x\n",
        "        return np.tanh(x)\n",
        "    \n",
        "    def backward_pass(self,upstream_gradient):\n",
        "        tan = np.tanh(self.x)\n",
        "        dx = 1 - np.power(tan,2)\n",
        "        return dx   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzoPLmRyAinQ",
        "colab_type": "text"
      },
      "source": [
        "### Loss layer\n",
        "It receives the output of the perceptron and calculates the loss, as well as the gradient passing trough the loss function. It also stores its history, so we can explore the cost in different stages of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEeX9aUHAinR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loss:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "    \n",
        "    def forward_pass(self, y_pred, y_true):\n",
        "        pass\n",
        "    \n",
        "    def backward_pass(self):\n",
        "        pass\n",
        "    \n",
        "    def initialize(self):\n",
        "        self.history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6HtsYmnAinV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RegressionLoss(Loss):\n",
        "    def forward_pass(self, y_pred, y_true):\n",
        "        a = np.square((y_pred-y_true)*(y_pred - y_true)).sum(axis = 1).mean()\n",
        "        return a\n",
        "    \n",
        "    def backward_pass(self,y_pred,y_true):\n",
        "        return (2/y_true.shape[0])* (y_pred - y_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCLfLBYRAinZ",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "## Data loading and normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4GVDhpEAina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"StudentsPerformance.csv\") \n",
        "categorical_vars = list(data.columns[0:5])\n",
        "df_raw = data[categorical_vars]\n",
        "df_temp = df_raw.astype(\"category\").apply(lambda x: x.cat.codes)\n",
        "data[categorical_vars] = df_temp.where(~df_raw.isna(), df_raw)\n",
        "continuous_vars = data.columns.difference(categorical_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFkr8koQ1nzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "data[continuous_vars]= min_max_scaler.fit_transform(data[continuous_vars])\n",
        "X = data.drop('test preparation course', axis = 1)\n",
        "X = X.astype(np.float64)\n",
        "y = data['test preparation course']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb5FfVzR2K_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_x = X.columns[1:3]\n",
        "enc = OneHotEncoder(sparse = False)\n",
        "a = enc.fit_transform(X[cat_x])\n",
        "a = pd.DataFrame(a)\n",
        "X = pd.concat([a,X], axis = 1)\n",
        "X = X.drop(cat_x, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqIQJbFWAini",
        "colab_type": "text"
      },
      "source": [
        "## Data splitting\n",
        "We need three sets of data: training, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iYzdT9VAinj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofx7H9_EAinn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M = X_train.shape[0] # Number of samples\n",
        "m = X_train.shape[1] # Number of features or entries\n",
        "n = m # number of outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT8Z6lXsAinr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 1 # Hidden layers\n",
        "i = 10 # Number of neurons in each hidden layer\n",
        "L = []\n",
        "inp = m\n",
        "activation_functions = [Sigmoid(), Sigmoid()]\n",
        "for j in range(l+1):\n",
        "    a = Local(inp,i)\n",
        "    L.append(Local(inp,i))\n",
        "    L.append(activation_functions[j])\n",
        "    inp = i\n",
        "L.append(Local(inp,n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlHtLMqAinv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = RegressionLoss()\n",
        "model = Model(L,loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbLFOYQjAin3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.5\n",
        "batch_size = 10\n",
        "epochs = 100\n",
        "tol = 1e-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDrRoCggwQgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93f3927f-1b0a-4df6-b330-9c971643d5c0"
      },
      "source": [
        "test_loss"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9573881742441371,\n",
              " 0.9570069381060567,\n",
              " 0.9563904308582013,\n",
              " 0.9552717598284608,\n",
              " 0.952886620824842,\n",
              " 0.9466493591306413,\n",
              " 0.9305538724361315,\n",
              " 0.9132354425066243,\n",
              " 0.9046880356411948,\n",
              " 0.9019198217089226,\n",
              " 0.9004678229682672,\n",
              " 0.8981394871661759,\n",
              " 0.8942644954190083,\n",
              " 0.8878312140336103,\n",
              " 0.8746177568670988,\n",
              " 0.8306822109065303,\n",
              " 0.7701090689243415,\n",
              " 0.7634342114781301,\n",
              " 0.7625517532488097,\n",
              " 0.7592260989620856,\n",
              " 0.7468909017590784,\n",
              " 0.6938330977296358,\n",
              " 0.6745280532487306,\n",
              " 0.6647865405006655,\n",
              " 0.6596845761891237,\n",
              " 0.652613400747413,\n",
              " 0.6410141014961701,\n",
              " 0.6138938986890905,\n",
              " 0.5937094873028405,\n",
              " 0.5937297574522441,\n",
              " 0.578407757035829,\n",
              " 0.53823535872764,\n",
              " 0.4675794882092354,\n",
              " 0.44305400531577627,\n",
              " 0.4380052271976574,\n",
              " 0.4005969577034598,\n",
              " 0.40440564916194705,\n",
              " 0.3902974472444172,\n",
              " 0.3924081315965556,\n",
              " 0.35979180036686137,\n",
              " 0.33153964163030464,\n",
              " 0.3155946251834402,\n",
              " 0.2975002724409555,\n",
              " 0.29623033322827963,\n",
              " 0.28802723717498513,\n",
              " 0.2867441607974298,\n",
              " 0.27920730041237085,\n",
              " 0.2710226838846201,\n",
              " 0.2606069312512501,\n",
              " 0.24456831433197063,\n",
              " 0.2216955855912546,\n",
              " 0.19065989248264212,\n",
              " 0.15932544859199418,\n",
              " 0.13306834962847824,\n",
              " 0.13085922543506903,\n",
              " 0.12307449005193766,\n",
              " 0.11576628086849221,\n",
              " 0.12597900783786792,\n",
              " 0.12154340868428258,\n",
              " 0.12247976086352619,\n",
              " 0.11681676736314837,\n",
              " 0.12226251915649294,\n",
              " 0.12023010670903705,\n",
              " 0.11526033699750406,\n",
              " 0.12037173036743891,\n",
              " 0.12113430772843359,\n",
              " 0.11852323547941035,\n",
              " 0.11472171112078171,\n",
              " 0.11728231418870197,\n",
              " 0.12252238983175788,\n",
              " 0.11476972741575438,\n",
              " 0.11072940475309641,\n",
              " 0.1101551728696319,\n",
              " 0.11422441265534342,\n",
              " 0.10871092099249254,\n",
              " 0.10224750476805591,\n",
              " 0.10028553753209649,\n",
              " 0.09712517959866515,\n",
              " 0.09525974054608709,\n",
              " 0.09168908452298911,\n",
              " 0.08866593734319345,\n",
              " 0.10165267409925066,\n",
              " 0.08407991381454898,\n",
              " 0.07874364653296603,\n",
              " 0.07344213552638859,\n",
              " 0.06988571227001955,\n",
              " 0.08032264338095771,\n",
              " 0.06255721353099408,\n",
              " 0.057474251984566925,\n",
              " 0.05496838325501632,\n",
              " 0.06132090024113508,\n",
              " 0.05106656778561826,\n",
              " 0.045611088862980924,\n",
              " 0.045413124599461466,\n",
              " 0.04605686561392861,\n",
              " 0.04070565127772783,\n",
              " 0.04707039029188729,\n",
              " 0.039105974709771486,\n",
              " 0.03782354818598744,\n",
              " 0.03655511653861734,\n",
              " 0.037132499696509995]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UPspRLlAin8",
        "colab_type": "text"
      },
      "source": [
        "## First learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SV694tsAin9",
        "colab_type": "code",
        "outputId": "7f450896-a8d0-4b2a-e42c-de3f18f6657c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_loss = model.fit(X_train.values, X_train.values, lr, epochs, batch_size, tol)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0: loss 3.943608102994729\n",
            "epoch 1: loss 1.198898595354663\n",
            "epoch 2: loss 1.1975461601911148\n",
            "epoch 3: loss 1.1952691483283997\n",
            "epoch 4: loss 1.19087590870666\n",
            "epoch 5: loss 1.1806036006774128\n",
            "epoch 6: loss 1.1520866304196382\n",
            "epoch 7: loss 1.0934249762743593\n",
            "epoch 8: loss 1.0482707665342403\n",
            "epoch 9: loss 1.0379893655170878\n",
            "epoch 10: loss 1.0384555154855115\n",
            "epoch 11: loss 1.039337313509037\n",
            "epoch 12: loss 1.0392195614750572\n",
            "epoch 13: loss 1.0381932319152698\n",
            "epoch 14: loss 1.0358575692549175\n",
            "epoch 15: loss 1.0307142545646935\n",
            "epoch 16: loss 0.9434336741846107\n",
            "epoch 17: loss 0.8722581770185099\n",
            "epoch 18: loss 0.8545005477263984\n",
            "epoch 19: loss 0.8460769837690514\n",
            "epoch 20: loss 0.8333484926789823\n",
            "epoch 21: loss 0.8168010604636099\n",
            "epoch 22: loss 0.7461847859545659\n",
            "epoch 23: loss 0.716754630918686\n",
            "epoch 24: loss 0.7061423961303838\n",
            "epoch 25: loss 0.6980119867325036\n",
            "epoch 26: loss 0.6870616735553716\n",
            "epoch 27: loss 0.6619685466599068\n",
            "epoch 28: loss 0.6297413634923525\n",
            "epoch 29: loss 0.5968865532062975\n",
            "epoch 30: loss 0.5647201954183312\n",
            "epoch 31: loss 0.5340734938176444\n",
            "epoch 32: loss 0.4828206724850172\n",
            "epoch 33: loss 0.4227362699457294\n",
            "epoch 34: loss 0.3857803090736625\n",
            "epoch 35: loss 0.3730846443790885\n",
            "epoch 36: loss 0.35090449074242513\n",
            "epoch 37: loss 0.3424040785187534\n",
            "epoch 38: loss 0.34658827083302973\n",
            "epoch 39: loss 0.3101055940846396\n",
            "epoch 40: loss 0.3015287707354427\n",
            "epoch 41: loss 0.27662335713860514\n",
            "epoch 42: loss 0.2630111355974457\n",
            "epoch 43: loss 0.26019984470406526\n",
            "epoch 44: loss 0.2420088580305764\n",
            "epoch 45: loss 0.2483868670347461\n",
            "epoch 46: loss 0.23367531366419808\n",
            "epoch 47: loss 0.22760428952029338\n",
            "epoch 48: loss 0.218982836149235\n",
            "epoch 49: loss 0.20807862809348368\n",
            "epoch 50: loss 0.1926683340094137\n",
            "epoch 51: loss 0.17255095542586607\n",
            "epoch 52: loss 0.15359328287455273\n",
            "epoch 53: loss 0.1389428589392344\n",
            "epoch 54: loss 0.12720565318630636\n",
            "epoch 55: loss 0.11207782505443348\n",
            "epoch 56: loss 0.1042703826555749\n",
            "epoch 57: loss 0.11234783440700687\n",
            "epoch 58: loss 0.10593391132623645\n",
            "epoch 59: loss 0.10481468866595658\n",
            "epoch 60: loss 0.09951045519067198\n",
            "epoch 61: loss 0.10120099806216754\n",
            "epoch 62: loss 0.093785572307341\n",
            "epoch 63: loss 0.093224996729742\n",
            "epoch 64: loss 0.09964104932542642\n",
            "epoch 65: loss 0.10427134491926598\n",
            "epoch 66: loss 0.09208634233624141\n",
            "epoch 67: loss 0.0912731232591154\n",
            "epoch 68: loss 0.09646302842401534\n",
            "epoch 69: loss 0.09084613941615068\n",
            "epoch 70: loss 0.0947413784611076\n",
            "epoch 71: loss 0.0870576009613778\n",
            "epoch 72: loss 0.09038028050216322\n",
            "epoch 73: loss 0.08511694209531306\n",
            "epoch 74: loss 0.09462018824679386\n",
            "epoch 75: loss 0.08196266616851164\n",
            "epoch 76: loss 0.0815460158618071\n",
            "epoch 77: loss 0.0793126259210632\n",
            "epoch 78: loss 0.07765802258965453\n",
            "epoch 79: loss 0.07556356070325373\n",
            "epoch 80: loss 0.07384718922453189\n",
            "epoch 81: loss 0.07331328985774195\n",
            "epoch 82: loss 0.08724563442655617\n",
            "epoch 83: loss 0.06781730783545181\n",
            "epoch 84: loss 0.06410048742369123\n",
            "epoch 85: loss 0.06152000547808831\n",
            "epoch 86: loss 0.0655852278685986\n",
            "epoch 87: loss 0.05991195289900631\n",
            "epoch 88: loss 0.05461855141527721\n",
            "epoch 89: loss 0.05187217007367749\n",
            "epoch 90: loss 0.053315443183328294\n",
            "epoch 91: loss 0.05356834716981889\n",
            "epoch 92: loss 0.04644163626440537\n",
            "epoch 93: loss 0.04478622766909938\n",
            "epoch 94: loss 0.04301765921988306\n",
            "epoch 95: loss 0.04145759553591708\n",
            "epoch 96: loss 0.05386781505441366\n",
            "epoch 97: loss 0.038866465044623634\n",
            "epoch 98: loss 0.03690792639269142\n",
            "epoch 99: loss 0.037525550466446046\n",
            "epoch 100: loss 0.03918226810200485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_oQm9AXAioD",
        "colab_type": "text"
      },
      "source": [
        "### Energy of the instant error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOATj2vTAioD",
        "colab_type": "code",
        "outputId": "444caf0a-2aa4-4f27-e47b-470f3c3672a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(loss.history, label = 'Training loss')\n",
        "plt.title('Loss history')\n",
        "plt.plot(test_loss, label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRMFse5QudWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49502783-2f34-4c1e-8bbc-edcb54a1d532"
      },
      "source": [
        "print('Test score: ', model.score(X_test, X_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score:  0.017611415739129145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_gFIDrvtvOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = []\n",
        "cv = KFold(n_splits=10, shuffle = True, random_state = 42)\n",
        "for train_index, test_index in cv.split(X_val):\n",
        "    loss = RegressionLoss()\n",
        "    model_ = Model(L,loss)\n",
        "    X_train_v, X_test_v = X_val.values[train_index], X_val.values[test_index]\n",
        "    model_.fit(X_train_v, X_train_v, lr, epochs, batch_size, tol, print_loss = False)\n",
        "    scores.append(model_.score(X_test_v, X_test_v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_QX4lZuLB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8159111c-58e5-4ee8-c2b7-14bea206842b"
      },
      "source": [
        "print('Cross validation score: ', np.mean(scores))\n",
        "print('Test score: ', model.score(X_test, X_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross validation score:  0.04487935954239422\n",
            "Test score:  0.03243881716618395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUattd-5mzTA",
        "colab_type": "text"
      },
      "source": [
        "## Extract the encoded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01fL5PMzmxy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_encoded = L[1].forward_pass(L[0].forward_pass(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-c-1SSqm6fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_encoded.to_csv('/content/students_encoded.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMnDcWxtnHLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}